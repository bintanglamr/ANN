# -*- coding: utf-8 -*-
"""ANN_05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sM1TD7kDecaWJ8okz_8lZKz7A4VykChB

# Install & Import Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (mean_squared_error, r2_score,
                             mean_absolute_error, explained_variance_score,
                             max_error)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
plt.rcParams["figure.figsize"] = (12,5)
import warnings
warnings.filterwarnings('ignore')

"""# Load Data"""

# Load and preprocess the dataset
df = pd.read_csv("/content/busan_dataset.csv")
df.columns = df.columns.str.strip()  # Remove spaces from column names
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)

"""# Data Exploration"""

# Display basic information and statistics
print(df.shape, df.info(), df.describe(), df.isna().sum(), sep='\n')

# Data Visualization
sns.lineplot(x=df.index, y='GHI_Average', data=df)
plt.title('GHI Average Over Time')
plt.show()

df_by_month = df.resample('M').sum()
sns.lineplot(x=df_by_month.index, y='GHI_Average', data=df_by_month)
plt.title('Monthly GHI Average')
plt.show()

# Feature Engineering
df['hour'] = df.index.hour
df['month'] = df.index.month
required_cols = ['GHI_Average', 'SunZenith_KMU', 'Ambient_Pressure',
                 'Water', 'AOD', 'Uo (atm-cm)', 'CI_Hammer', 'OT', 'hour', 'month']
df = df[required_cols]

df.head()

# Import the required library for color mapping
from itertools import cycle

# Select the last 100 timesteps from the DataFrame
df_last_100 = df.tail(100)

# Create a figure with multiple subplots based on the number of variables
plt.figure(figsize=(15, 20))

# Define a color cycle from the 'tab10' colormap to assign different colors to each variable
color_cycle = cycle(plt.cm.tab10.colors)

# Loop through each column and create a line plot with a unique color
for i, column in enumerate(df_last_100.columns, 1):
    plt.subplot(len(df_last_100.columns), 1, i)  # Create a subplot for each variable
    plt.plot(df_last_100.index, df_last_100[column], label=column, color=next(color_cycle))  # Use different colors
    plt.title(f'Line Graph for {column} - Last 100 Data Points')
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.legend(loc="upper right")

# Adjust layout to prevent overlap
plt.tight_layout()

# Display the plot
plt.show()

# Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='seismic', linewidths=0.75, fmt=".2f")
plt.title('Correlation Matrix of Input Variables')
plt.show()

"""# Data Preprocessing"""

# Split the dataset
train_ratio, test_ratio, validation_ratio = 0.7, 0.15, 0.15
train_size = int(len(df) * train_ratio)
test_size = int(len(df) * test_ratio)
train, test, validation = df.iloc[:train_size], df.iloc[train_size:train_size + test_size], df.iloc[train_size + test_size:]

# Input Scaling
cols = ['SunZenith_KMU', 'Ambient_Pressure', 'Water', 'AOD',
        'OT', 'Uo (atm-cm)', 'CI_Hammer', 'hour', 'month']

scaler = RobustScaler()
train[cols] = scaler.fit_transform(train[cols])
test[cols] = scaler.transform(test[cols])

# scaling GHI
GHI_scaler = RobustScaler()
GHI_scaler = GHI_scaler.fit(train[['GHI_Average']])
train['GHI_Average'] = GHI_scaler.transform(train[['GHI_Average']])
test['GHI_Average'] = GHI_scaler.transform(test[['GHI_Average']])

print(f'Train size: {len(train)}, Test size: {len(test)}, Validation size: {len(validation)}')

"""# Model Building"""

# Model Building
def create_dataset(X, y, time_steps=1):
    """Creates dataset for training/testing"""
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X.iloc[i + time_steps - 1].values)  # Previous time step features
        ys.append(y.iloc[i + time_steps])  # Target variable
    return np.array(Xs), np.array(ys)

# Including GHI_Average as an input variable
time_steps = 1  # Adjust time_steps as needed
X_ANN_train, y_ANN_train = create_dataset(train, train['GHI_Average'], time_steps)
X_ANN_test, y_ANN_test = create_dataset(test, test['GHI_Average'], time_steps)

print(f'X_ANN_train shape: {X_ANN_train.shape}, y_ANN_train shape: {y_ANN_train.shape}')

"""# ANN"""

# Build the ANN model
ann_model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=100, activation='relu', input_shape=(X_ANN_train.shape[1],)),
    tf.keras.layers.Dropout(rate=0.5),
    tf.keras.layers.Dense(units=1)
])

ann_model.compile(loss='mean_squared_error', optimizer='adam')
ann_model.summary()

# Train the model
ann_history = ann_model.fit(X_ANN_train, y_ANN_train, epochs=100,
                             batch_size=64, validation_split=0.15, shuffle=False)

# Loss visualization
plt.plot(ann_history.history['loss'], label='train')
plt.plot(ann_history.history['val_loss'], label='validation')
plt.title('ANN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Predictions and inverse scaling
y_pred_ANN = ann_model.predict(X_ANN_test)
y_pred_ANN_inv = GHI_scaler.inverse_transform(y_pred_ANN.reshape(-1, 1))
y_ANN_test_inv = GHI_scaler.inverse_transform(y_ANN_test.reshape(-1, 1))

# Visualization of predictions
plt.plot(y_ANN_test_inv.flatten(), marker='.', label='True')
plt.plot(y_pred_ANN_inv.flatten(), 'r', label='Predicted')
plt.title('ANN Model Prediction')
plt.ylabel('GHI')
plt.xlabel('Time')
plt.legend()
plt.show()

# Evaluation
def ANN_accuracy_metrics(y_true, y_pred):
    """Calculate and print model performance metrics"""
    y_true_flat, y_pred_flat = y_true.flatten(), y_pred.flatten()
    metrics = {
        'R^2': r2_score(y_true_flat, y_pred_flat),
        'MAE': mean_absolute_error(y_true_flat, y_pred_flat),
        'MSE': mean_squared_error(y_true_flat, y_pred_flat),
        'RMSE': np.sqrt(mean_squared_error(y_true_flat, y_pred_flat)),
        'MBE': np.mean(y_true_flat - y_pred_flat),
        'RRMSE': np.sqrt(mean_squared_error(y_true_flat, y_pred_flat)) / np.mean(y_true_flat),
        'RMBE': np.mean(y_true_flat - y_pred_flat) / np.mean(y_true_flat),
    }

    # Print the metrics
    print("\nANN Accuracy Metrics")
    print("-------------------")
    for metric, value in metrics.items():
        print(f'{metric}: {value:.4f}')

# Call the function to compute and print ANN Accuracy Metrics
ANN_accuracy_metrics(y_ANN_test_inv, y_pred_ANN_inv)